{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe6yTh55trpQ"
   },
   "source": [
    "# Assignment 1, Task 1: Basic Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7OSce2LHK_B"
   },
   "source": [
    "In this task, you are going to implement two classifers and apply them to the  Fashion-MNIST dataset: \n",
    "\n",
    "(1) Logistic regression classifier\n",
    "\n",
    "(2) Softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLE5oHAkHK_C"
   },
   "outputs": [],
   "source": [
    "# Import modules, make sure you have installed all required packages before you start.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Module auto reloading. (Ref: https://ipython.org/ipython-doc/3/config/extensions/autoreload.html)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMZSC2GLHK_F"
   },
   "source": [
    "## Load Fashion-MNIST data\n",
    "\n",
    "Fashion-MNIST is a widely used dataset mainly used for benchmarking the very basic machine learning models. Images are drawn from Zalando's clothing articles and the dataset consists of a training set with 60,000 examples and a test set with 10,000 examples. Each example is a 28x28 pixel grayscale image with an associated label from 10 classes. We will use this to create our training set, validation set, and test set.\n",
    "\n",
    "See https://github.com/zalandoresearch/fashion-mnist for more details on Fashion-MNIST. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-class dataset Fashion-MNIST\n",
    "\n",
    "First, we load the raw Fashion-MNIST data to create a 10-class dataset and manually define a label map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7274,
     "status": "ok",
     "timestamp": 1559884655914,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "bV0hpeEqHK_G",
    "outputId": "c4b54f60-33c3-4d00-c783-316c1de30020",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, test = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_test_raw, y_test = test\n",
    "\n",
    "# the integer labels in y_train and y_test correspond to the index of this label map\n",
    "label_map = [\n",
    "    't-shirt/top', 'trouser', 'pullover', 'dress', 'coat', \n",
    "    'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'\n",
    "]\n",
    "\n",
    "# Here we vectorize the data (rearranged the storage of images) for you. \n",
    "# That is, we flatten 1×28×28 images into 1×784 Numpy arrays.\n",
    "# The reason we do this is because we can not put 2-D image representations into our model. \n",
    "# This is common practice (flattening images before putting them into the ML models). \n",
    "# Note that this practice may not be used for Convolutional Neural Networks (CNN). \n",
    "# We will later see how we manage the data when used in CNNs in later assignments.\n",
    "\n",
    "# Check the results\n",
    "print('Raw training data shape: ', X_train_raw.shape)\n",
    "print('Raw test data shape: ', X_test_raw.shape)\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_test = X_test_raw.reshape((X_test_raw.shape[0], X_test_raw.shape[1]**2))\n",
    "\n",
    "num_classes = max(y_train) + 1\n",
    "\n",
    "print('Number of classes: ', num_classes)\n",
    "print('Vectorized training data shape: ', X_train.shape)\n",
    "print('Vectorized test data shape: ', X_test.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8435,
     "status": "ok",
     "timestamp": 1559884658097,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "6n4v89BcHK_L",
    "outputId": "9141d16e-b67a-4d60-a7ed-0eb350bc4748",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to reshape vectorized data into the image format for visualization\n",
    "X = X_train.reshape(X_train.shape[0], X_train_raw.shape[1], X_train_raw.shape[2])\n",
    "\n",
    "print(label_map)\n",
    "print(X.shape)\n",
    "\n",
    "#Visualizing Fashion-MNIST data. We randomly choose 25 images from the train dataset.\n",
    "fig, axes1 = plt.subplots(5, 5, figsize=(8, 8))\n",
    "for j in range(5):\n",
    "    for k in range(5):\n",
    "        i = np.random.choice(range(len(X)))\n",
    "        axes1[j][k].set_axis_off()\n",
    "        axes1[j][k].imshow(X[i:i+1][0], cmap='gray')\n",
    "        axes1[j][k].set_title(label_map[y_train[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1559884658978,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "6YoejUt3HK_O",
    "outputId": "8f961d92-4960-4fb2-d496-5e0521996c7a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data organization:\n",
    "#    Train data: 49,000 samples from the original train set: indices 1~49,000\n",
    "#    Validation data: 1,000 samples from the original train set: indices 49,000~50,000\n",
    "#    Test data: 1,000 samples from the original test set: indices 1~1,000\n",
    "#    Development data (for gradient check): 100 random samples from the train set: indices 1~49,000\n",
    "#    Development data (binary) (only for gradient check in Part 1): 100 random samples from the subsampled binary train set\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 100\n",
    "num_dev_binary = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "X_test = X_test[:num_test, :]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-class dataset (Subsampled from Fashion-MNIST)\n",
    "\n",
    "Next, in order to implement the experiment with the logistic regression classifier, we subsample the 10-class dataset to the 2-class dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subsample 10-class training set to 2-class training set\n",
    "X_train_binary = X_train[y_train<2,:]\n",
    "num_training_binary = X_train_binary.shape[0]\n",
    "y_train_binary = y_train[y_train<2]\n",
    "mask_binary = np.random.choice(num_training_binary, num_dev_binary, replace=False)\n",
    "\n",
    "X_val_binary = X_val[y_val<2,:]\n",
    "y_val_binary = y_val[y_val<2]\n",
    "\n",
    "X_dev_binary = X_train_binary[mask_binary]\n",
    "y_dev_binary = y_train_binary[mask_binary]\n",
    "\n",
    "print('Train data (binary) shape: ', X_train_binary.shape)\n",
    "print('Train labels (binary) shape: ', y_train_binary.shape)\n",
    "print('Validation data (binary) shape: ', X_val_binary.shape)\n",
    "print('Validation labels (binary) shape: ', y_val_binary.shape)\n",
    "print('Development data (binary) shape:', X_dev_binary.shape)\n",
    "print('Development labels (binary) shape', y_dev_binary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Training data is 2D with shape (batch, dim)\n",
    "* Subtract the mean value of all the dim, i.e. the mean should be computed along the batch (0-th) dimension\n",
    "\n",
    "<font color=\"red\"><strong>NOTE</strong></font>: We always use the statistics from the training set when normalizing validation & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2569,
     "status": "ok",
     "timestamp": 1559884662531,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "XWboKTBmHK_S",
    "outputId": "a81860fe-983b-44c7-8382-0b68b396815f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the 10-class dataset\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "# For the binary dataset\n",
    "mean_image = np.mean(X_train_binary, axis=0)\n",
    "\n",
    "X_train_binary = X_train_binary.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val_binary = X_val_binary.astype(np.float32) - mean_image\n",
    "X_dev_binary = X_dev_binary.astype(np.float32) - mean_image\n",
    "\n",
    "print('Train data mean: ', np.mean(X_train))\n",
    "print('Validation data mean: ', np.mean(X_val))\n",
    "print('Test data mean: ', np.mean(X_test))\n",
    "print('Train data (binary) mean: ', np.mean(X_train_binary))\n",
    "print('Validation data (binary) mean: ', np.mean(X_val_binary))\n",
    "print('Development data (binary) mean: ', np.mean(X_dev_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Basics (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are going to get familiar with one of the very basic modeling methodologies of classification problems. \n",
    "\n",
    "<font color=\"red\"><strong>NOTE</strong></font>: Without further specification, all vectors mentioned are considered column vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume a training dataset $X$ of $N$ images that all have shape $(H, W, C)$, then $X \\in R^{N \\times D}$ where $D = H \\times W \\times C$. \n",
    "\n",
    "Consider a *flattened* image $x_i \\in X$ with an associated label $y_i = \\{1 \\dots K\\}$, where $i = 1 \\dots N$ and $K$ represents the number of distinct classes. We have that $x_i \\in R^D$, $y_i \\in R$ and $y = (y_1, \\dots, y_N)^T \\in R^N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Fashion-MNIST dataset we have training data `X_train` with shape $(49000, 784)$ and the corresponding labels `y_train` with shape $(49000,)$.\n",
    "\n",
    "Here the number of input images $N = 49000$, and each image has a flattened dimension: $D = 28 \\times 28 \\times 1 = 784$. The total number of classes $K = 10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some $x_i \\in R^D$, we will now define the score function $f(\\cdot): R^D \\to R^K$ that maps \n",
    "the $D$ pixels to $K$ different class scores:\n",
    "\n",
    "$$f(x_i; W, b) = W^T x_i + b$$\n",
    "\n",
    "where $W \\in R^{D \\times K}$ is a linear map and $b \\in R^K$ represents the biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the **bias trick** to represent the two parameters ($W, b$) as one. Consider a concatenation on the data point $x_i$ and the weights $W$ given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_i &\\gets[x_i; 1] \\in R^{D + 1} \\\\ \n",
    "W &\\gets [W; b] \\in R^{(D + 1) \\times K}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which translates to:\n",
    "\n",
    "- Extend the vector $x_i$ with one additional dimension that always holds the constant $1$\n",
    "- Combine the original weights $W$ and biases $b$ to form a new weight matrix\n",
    "\n",
    "The new score function will then simplify to a single matrix multiplication: \n",
    "\n",
    "$$f_i = f(x_i;W)=W^T x_i$$\n",
    "\n",
    "Or equivalantly, \n",
    "\n",
    "$$f = \\begin{bmatrix} f_1^T \\\\ \\vdots \\\\ f_N^T \\end{bmatrix} = X W$$\n",
    "\n",
    "where $f \\in R^{N \\times K}$ are the class scores (logits) of all $K$ classes for each of the $N$ data points. Recall that $X = \\begin{bmatrix} x_1^T \\\\ \\vdots \\\\ x_N^T \\end{bmatrix}$. \n",
    "\n",
    "For our data, $D + 1 = 784 + 1 = 785$ and the new weights $W \\in R^{785 \\times 10}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>:  Implement the bias trick by extending the feature vectors. In our case this involves adding a bias dimension of ones to each of `X_train`, `X_val`, `X_test`, `X_dev`, `X_train_binary`, `X_val_binary`, and `X_dev_binary`.\n",
    "\n",
    "<font color=\"red\"><strong>HINT</strong></font>: Make your life easier with `np.hstack`. https://numpy.org/doc/stable/reference/generated/numpy.hstack.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# TODO: Append the bias dimension of ones (i.e. bias trick) so that our models\n",
    "# only have to worry about optimizing a single weight matrix W.\n",
    "\n",
    "\n",
    "\n",
    "# END of your code\n",
    "#############################################################\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"X_dev shape: {}\".format(X_dev.shape))\n",
    "print(\"X_train_binary shape: {}\".format(X_train_binary.shape))\n",
    "print(\"X_val_binary shape: {}\".format(X_val_binary.shape))\n",
    "print(\"X_dev_binary shape: {}\".format(X_dev_binary.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Logistic Regression Classifier (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are going to implement a logistic regression classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression classifier can solve a binary classification problem that has only two classes: $(0, 1)$. Therefore, it is reasonable for us to model only one of the two classes, and assign whatever else to the other class[3]. \n",
    "\n",
    "For some data $x_i \\in X$ with corresponding label $y_i = \\{0, 1\\}$, we study the likelihood of this data point belonging to class \"$1$\", i.e. \n",
    "\n",
    "$$P(y_i = 1)$$ \n",
    "\n",
    "Here, we set $K = 1$ (because we're only modeling one class) so that we have a $1$-dimensional vector as the model weights (as opposed to a matrix descrbied in ***Part 0***): \n",
    "\n",
    "$$W \\in R^{(D + 1) \\times 1} \\to w \\in R^{D + 1}$$\n",
    "\n",
    "Hence the logit for $x_i$ reads \n",
    "\n",
    "$$f_i = w^T x_i \\in R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the likelihood of $x_i$ belonging to class $1$: \n",
    "\n",
    "$$P(y_i = 1 | x_i; w) = \\frac{1}{1 + e^{-w^T x_i}} = \\frac{1}{1 + e^{-f_i}} = \\sigma (f_i)$$\n",
    "\n",
    "where $\\sigma (\\cdot): R \\to (0, 1)$ is the `sigmoid` function. \n",
    "\n",
    "Thus the likelihood for class $0$ follows \n",
    "\n",
    "$$P(y_i = 0 | x_i; w) = 1 - P(y_i = 1 | x_i; w) = 1 - \\sigma (f_i)$$\n",
    "\n",
    "so that the probabilities of class $1$ and class $0$ sum up to one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, by setting a probability threashold $T = 0.5$, an example $x_i$ is classified as a positive example ($y_i = 1$) if \n",
    "\n",
    "$$\\sigma_i = \\sigma (f_i) > T$$\n",
    "\n",
    "or equivalently if the score $f_i > 0$, and vice versa[3]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the joint classification likelihood for class $1$ and class $0$ for some $x_i$:\n",
    "\n",
    "$$\n",
    "P(y_i | x_i; w) = \\begin{cases} \n",
    "\\sigma_i & \\text{if } y_i = 1 \\\\ \n",
    "1 - \\sigma_i & \\text{else} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which can then be written in a more compact form[3]: \n",
    "\n",
    "$$P_i = \\sigma_i^{y_i} (1 - \\sigma_i)^{1 - y_i}$$\n",
    "\n",
    "The likelihood provides in general a probability of our model making a correct prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the \"goodness\" of our predictions, we define the classification loss on $x_i$ using the ***negative*** of the log likelihood: \n",
    "\n",
    "$$L_i = -\\log P_i = -y_i \\log \\sigma_i - (1 - y_i) \\log (1 - \\sigma_i)$$\n",
    "\n",
    "Ultimately, we take the average over all samples for $i = 1 \\dots N$ and add a regularization term:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_i L_i + \\frac{\\lambda}{2} \\|w\\|_2^2$$\n",
    "\n",
    "where $\\lambda$ is the regularization factor. \n",
    "\n",
    "This is an $L_2$ regularization, meaning that the regularization term is calculated by the $L_2$ norm of the coefficient $w$. Recall that\n",
    "\n",
    "$$\\|w\\|_2 = \\sqrt {w_1^2 + w_2^2 + \\dots + w_K^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the model can be learned by minimizing the loss function (i.e. maximizing the likelihood): \n",
    "\n",
    "$$w^* = \\arg\\min_w L$$\n",
    "\n",
    "which can be achieved by gradient descent. The gradient of $L$ (and each $L_i$) w.r.t $w$ can be nicely formulated as \n",
    "\n",
    "$$\\frac{\\partial L_i}{\\partial w} = -(y_i - \\sigma_i) x_i$$\n",
    "\n",
    "and therefore, \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = -\\frac{1}{N} \\sum_i (y_i - \\sigma_i) x_i + \\lambda w$$\n",
    "\n",
    "Or equivalently, \n",
    "\n",
    "$$\\nabla_w L = -\\frac{1}{N} X^T (y - \\sigma) + \\lambda w$$\n",
    "\n",
    "where $\\sigma = (\\sigma_1, \\dots, \\sigma_N)^T \\in R^N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://cs231n.github.io/neural-networks-2/\n",
    "\n",
    "[2] https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\n",
    "\n",
    "[3] Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York: Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_sQKxgGHK_V"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Based on the derivations given above, complete the code in **./utils/classifiers/logistic_regression.py**. You have to implement the classifier in two ways: \n",
    "\n",
    "* Naive method using for-loop\n",
    "* Vectorized method\n",
    "\n",
    "We provide the verification code for you to check if your implementation is correct. \n",
    "\n",
    "***Do not forget the $L_2$ regularization term in the loss.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR in Tensorflow is demonstrated in the verification code below. This step will familiarize you with TensorFlow functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1559884668395,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "v0DB0KWoHK_Y",
    "outputId": "eb47b0b6-6b34-4625-9a24-03dc219914fd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verification code for checking the correctness of the implementation of logistic_regression\n",
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT\n",
    "\n",
    "from utils.classifiers.logistic_regression import logistic_regression_loss_naive\n",
    "from utils.classifiers.logistic_regression import logistic_regression_loss_vectorized\n",
    "\n",
    "# generate initial weight vector\n",
    "w = np.random.randn(X_train.shape[1]) * 0.0001\n",
    "reg = 0.000005\n",
    "\n",
    "## naive numpy implementation of Logistic Regression\n",
    "loss_naive, grad_naive = logistic_regression_loss_naive(w, X_dev_binary, y_dev_binary, reg)\n",
    "\n",
    "## vectorized numpy implementation of Logistic Regression\n",
    "loss_vec, grad_vec = logistic_regression_loss_vectorized(w, X_dev_binary, y_dev_binary, reg)\n",
    "\n",
    "## true value computed by tf\n",
    "# here we specify float64 because this is the default float type in numpy, so\n",
    "# an usual float32 may result in inconsistancy due to floating point error\n",
    "w_tf = tf.Variable(w, dtype=tf.float64)\n",
    "with tf.GradientTape() as tape:\n",
    "     tape.watch(w_tf)\n",
    "     loss_true = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          tf.Variable(y_dev_binary, dtype=tf.float64), \n",
    "          tf.linalg.matvec(tf.Variable(X_dev_binary, dtype=tf.float64), w_tf)\n",
    "     )) + reg * tf.nn.l2_loss(w_tf)\n",
    "     grad_true = tape.gradient(loss_true, w_tf)\n",
    "\n",
    "## check the correctness\n",
    "print('naive numpy loss: {}.'.format(loss_naive))\n",
    "print('vectorized numpy loss: {}.'.format(loss_vec))\n",
    "print('true loss: {}'.format(loss_true))\n",
    "print('*'*100)\n",
    "print('Relative naive gradient error is {}'.format(np.linalg.norm(grad_naive - grad_true)))\n",
    "print('Relative vectorized gradient error is {}'.format(np.linalg.norm(grad_vec - grad_true)))\n",
    "print('*'*100)\n",
    "print('Is naive loss correct? {}'.format(np.allclose(loss_naive, loss_true)))\n",
    "print('Is naive gradient correct? {}'.format(np.allclose(grad_naive, grad_true)))\n",
    "print('Is vectorized loss correct? {}'.format(np.allclose(loss_vec, loss_true)))\n",
    "print('Is vectorized gradient correct? {}'.format(np.allclose(grad_naive, grad_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Softmax Classifier (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax classifier generalizes the logistic regression classifier to multiple classes.\n",
    "\n",
    "In the softmax classifier, the function mapping \n",
    "\n",
    "$$f(X; W) = X W: R^{N \\times (D + 1)} \\to R ^ {N \\times K}$$\n",
    "\n",
    "stays unchanged, but we now interpret the scores $f$ as the predicted probability distribution (unnormalized) over all classes for each data point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to measure the goodness of our predictions, we replace the logistic regression loss (defined on $R$) with a cross-entropy loss (defined on $R^K$). For some (discrete) distributions $p, q \\in R^K$, the similarity between them can be measured by the cross entropy: \n",
    "\n",
    "$$H(p, q) = -\\sum_k p_k \\log q_k: \\Delta_K \\times \\Delta_K \\setminus \\{0, 1\\} \\to R_+$$\n",
    "\n",
    "where $\\Delta_K$ denotes the ***simplex*** in $R^K$: \n",
    "\n",
    "$$\\Delta_K = \\{x \\in R^K: \\sum_{i = 1}^K x_i = 1, x_i \\ge 0 \\text{ for } i = 1, \\dots, K \\}$$\n",
    "\n",
    "and the symbol \"$\\setminus$\" excludes the endpoints $\\{0, 1\\}$ from the domain of $q$. The definition of simplex corresponds to the notion of a probability distribution. \n",
    "\n",
    "Similarly, define the `softmax` function \n",
    "\n",
    "$$\\sigma (s) = \\frac{e^s}{\\sum_k e^{s_k}}: R^K \\to \\Delta_K \\setminus \\{0, 1\\}$$\n",
    "\n",
    "which is just a `sigmoid` function generalized to $R^K$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some $x_i \\in X$ and corresponding label $y_i \\in y$, the loss is defined by the cross entropy: \n",
    "\n",
    "$$L_i = H(p_i, \\sigma_i) = -\\sum_k p_{i, k} \\log \\sigma_{i, k} = -p_i^T \\log \\sigma_i$$\n",
    "\n",
    "where $p_i=[0, \\dots,1, \\dots, 0] \\in R^K$ is the ground truth distribution that contains a single $1$ at the $y_i$-th position (this is called a one-hot encoding), and \n",
    "\n",
    "$$\\sigma_i = \\sigma (W^T x_i) \\in R^K$$\n",
    "\n",
    "is the predicted (normalized) class distribution of $x_i$ w.r.t all $K$ classes. \n",
    "\n",
    "You can intuitively interprete the loss as a description of how far away your predicted distribution $\\sigma$ is from the actual groud truth distribution $p$. \n",
    "\n",
    "Note that $W = (w_1, \\dots, w_K) \\in R^{(D + 1) \\times K}$, $w_k \\in R^{D + 1}$ is the mapping specifically on class $k$ and $\\sigma_{i, k} = \\sigma_i[k] = \\sigma (w_k^T x_i)$ is the score of $x_i$ on class $k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u9-MqILHK_b"
   },
   "source": [
    "Thus the overall loss can be similarly given as: \n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_i L_i + \\frac{\\lambda}{2} \\|W\\|_F^2$$\n",
    "\n",
    "where $\\|W\\|_F = \\sqrt{\\sum_{i, j} W_{ij}^2}$ is the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm), and $\\|W\\|_F^2 = \\sum_k \\|w_k\\|_2^2$ which corresponds to the $L_2$ norms with respect to the weights of each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the model learns by\n",
    "\n",
    "$$W^* = \\arg\\min_W L$$\n",
    "\n",
    "And the gradient w.r.t $w_k$ follows  \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_k} = - \\frac{1}{N} \\sum_i (p_{i,k} - \\sigma_{i, k}) x_i + \\lambda w_k$$\n",
    "\n",
    "Or equivalently, \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = -\\frac{1}{N} X^T (P - \\sigma) + \\lambda W$$\n",
    "\n",
    "where $P = (p_1, \\dots, p_N)^T \\in R^{N \\times K}$ and $\\sigma = (\\sigma_1, \\dots, \\sigma_N)^T \\in R^{N \\times K}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: **Numerical Stability**. When you are writing code for computing the `softmax` function in practice, the intermediate terms $e^s$ and $\\sum_k e^{s_k}$ may be very large due to the exponentials. Division with large numbers can be numerically unstable, so it is important to use the normalization trick. \n",
    "\n",
    "Notice that if we multiply both the top and the bottom of the fraction by constant $C$ and push $C$ inside the exponent, we get the following (mathematically equivalent) expression: \n",
    "\n",
    "$$\n",
    "\\sigma (s)\n",
    "= \\frac{e^s}{\\sum_k e^{s_k}}\n",
    "= \\frac{C e^s}{C \\sum_k e^{s_k}}\n",
    "= \\frac{e^{s + \\log C}}{\\sum_k e^{s_k + \\log C}}\n",
    "$$\n",
    "\n",
    "A common choice for $C$ is to set $\\log C= -\\max_i s_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7u9-MqILHK_b"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Based on the derivations above, complete the code in **./utils/classifiers/softmax.py**. You have to implement the classifier in two ways: \n",
    "\n",
    "* Naive method using for-loop\n",
    "* Vectorized method\n",
    "\n",
    "We provide the verification code for you to check if your implementation is correct. \n",
    "\n",
    "***Do not forget the $L_2$ regularization term in the loss.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax in Tensorflow is demonstrated in the verification code below. This step will familiarize you with TensorFlow functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1559884674707,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "DH7HdbEAHK_b",
    "outputId": "a6d3eb81-af5a-427e-f25e-4c811e36d847",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verification code for checking the correctness of the implementation of softmax implementations\n",
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.softmax import softmax_loss_naive\n",
    "from utils.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "## generate a random weight matrix of small numbers\n",
    "#np.random.seed(3456)\n",
    "W = np.random.randn(X_train.shape[1], num_classes) * 0.0001\n",
    "reg = 0.000005\n",
    "\n",
    "## naive softmax in numpy\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive numpy loss: {}, takes {} seconds.'.format(loss_naive, toc - tic))\n",
    "\n",
    "## vectorized softmax in numpy\n",
    "tic = time.time()\n",
    "loss_vec, grad_vec = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized numpy loss: {}, takes {} seconds.'.format(loss_vec, toc - tic))\n",
    "\n",
    "# true value computed by tf\n",
    "W_tf = tf.Variable(W, dtype = tf.float64)\n",
    "tic = time.time()\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(W_tf)\n",
    "    loss_true = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        tf.one_hot(tf.Variable(y_dev, dtype=tf.int32), num_classes), \n",
    "        tf.matmul(tf.Variable(X_dev, dtype=tf.float64), W_tf)\n",
    "    )) + reg * tf.nn.l2_loss(W_tf)\n",
    "    grad_true = tape.gradient(loss_true, W_tf)\n",
    "toc = time.time()\n",
    "print('true loss: {}, takes {} seconds'.format(loss_true, toc - tic))\n",
    "\n",
    "## check the correctness\n",
    "print('*'*100)\n",
    "print('Relative loss error of naive softmax is {}'.format(np.linalg.norm(loss_true - loss_naive)))\n",
    "print('Relative loss error of vectorized softmax is {}'.format(np.linalg.norm(loss_true - loss_vec)))\n",
    "print('Gradient error of naive softmax is {}'.format(np.linalg.norm(grad_true - grad_naive)))\n",
    "print('Gradient error of vectorized softmax is {}'.format(np.linalg.norm(grad_true - grad_vec)))\n",
    "print('*'*100)\n",
    "print('Is naive softmax loss correct? {}'.format(np.allclose(loss_true, loss_naive)))\n",
    "print('Is vectorized softmax loss correct? {}'.format(np.allclose(loss_true, loss_vec)))\n",
    "print('Is naive softmax grad correct? {}'.format(np.allclose(grad_true, grad_naive,1e-02)))\n",
    "print('Is vectorized softmax grad correct? {}'.format(np.allclose(grad_true, grad_vec,1e-02)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train your classifiers (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1e7981-XHK_f"
   },
   "source": [
    "Now you can start to train your classifiers. Due to the fact that both the logistic regression and softmax classifer does not admit an analytical solution, we are going to use gradient descent algorithm for training. \n",
    "\n",
    "In the training section, you are asked to implement gradient descent optimization method, which can be interpreted as an attempt to minimize the loss function following iterative update of the model parameters using \n",
    "\n",
    "$$w \\gets w - \\alpha \\nabla_w L$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $\\nabla_w L$ is the gradient of the loss $L$ w.r.t the coefficient $w$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: The original code is given in **./utils/classifiers/basic_classifier.py**. You need to complete functions `train` and `predict`, in the class `BasicClassifier`. Later, you use its subclasses `LogisticRegression` and `Softmax` to train the model seperately and verify your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H35ZjRpZHK_g"
   },
   "source": [
    "### Train Logistic Regression + Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the code of subclasses `LogisticRegression` in **./utils/classifiers/basic_classifiers.py**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11111,
     "status": "ok",
     "timestamp": 1559884688501,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "DjRy76pdHK_h",
    "outputId": "ca2707c6-3adf-4bc8-bbce-d30e2a0f0e59",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.basic_classifiers import LogisticRegression\n",
    "\n",
    "## Logistic Regression + SGD\n",
    "classifier = LogisticRegression()\n",
    "reg = 1e-3 # regularization\n",
    "lr = 1e-5 # learning rate\n",
    "loss_hist_sgd = classifier.train(\n",
    "    X_train_binary, y_train_binary, \n",
    "    learning_rate=lr, reg=reg, \n",
    "    num_iters=300, optim='SGD', \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance \n",
    "# on both training set and validation set\n",
    "y_train_pred = classifier.predict(X_train_binary)\n",
    "print('training accuracy:', np.mean(y_train_binary == y_train_pred))\n",
    "y_val_pred = classifier.predict(X_val_binary)\n",
    "print('validation accuracy:', np.mean(y_val_binary == y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1559884692553,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "E_QGDSgvHK_k",
    "outputId": "70dd1a9c-3603-4faf-d8d2-625fbf050c20"
   },
   "outputs": [],
   "source": [
    "## SGD error plot\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HImk5vMHK_n"
   },
   "source": [
    "### Train Softmax + SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the code of subclasses **Softmax** in **./utils/classifiers/basic_classifier.py**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11222,
     "status": "ok",
     "timestamp": 1559884706216,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "CR4ZDYF6HK_o",
    "outputId": "02212126-294d-4e77-c7d6-b23b94593ab2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.basic_classifiers import Softmax\n",
    "\n",
    "## Softmax + SGD\n",
    "classifier = Softmax()\n",
    "reg = 1e-3 # regularization\n",
    "lr = 1e-5 # learning rate\n",
    "loss_hist_sgd = classifier.train(\n",
    "    X_train, y_train, \n",
    "    learning_rate=lr, reg=reg, \n",
    "    num_iters=300, optim='SGD', \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Write the BasicClassifier.predict function and evaluate the performance \n",
    "# on both the training and validation set\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1559884709159,
     "user": {
      "displayName": "Huixiang Zhuang",
      "photoUrl": "",
      "userId": "14481990962835760752"
     },
     "user_tz": 240
    },
    "id": "SCl-qjtlHK_r",
    "outputId": "0d82c109-b1c9-43ea-90c5-6bc6084751af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SGD loss curve\n",
    "plt.plot(loss_hist_sgd, label='SGD')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task1-basic_classifiers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
