{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe6yTh55trpQ"
   },
   "source": [
    "# Assignment 1, Task 2: Multilayer Perceptron (MLP)\n",
    "You will get to know how to build basic fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Vs2WYIFtrpS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gYnTjputrpV"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "I31uJ6KltrpW",
    "outputId": "1a677958-43ca-422c-ec66-38e8cdff4283",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, test = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_test_raw, y_test = test\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_test = X_test_raw.reshape((X_test_raw.shape[0], X_test_raw.shape[1]**2))\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49,000\n",
    "# Validation data: 1000 samples from original train set: 49,000~50,000\n",
    "# Test data: 10000 samples from original test set: 1~10,000\n",
    "# Development data (for gradient check): 100 from the train set: 1~49,000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLhMsud9trpa"
   },
   "source": [
    "## Part 1: Basic Layers (15%)\n",
    "\n",
    "In this part, all the functions will be created from scratch using numpy for better understanding (you will be introduced to built in layers from TensorFlow in the next task). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create basic layer functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an affine layer with depth $M$, for input $X \\in R^{N \\times D}$, the forward pass (with weights $W \\in R^{D \\times M}$ and bias $b \\in R^M$): \n",
    "\n",
    "$$Y = XW + \\mathbb{1} b^T \\in R^{N \\times M}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete \n",
    "\n",
    "1. The following equations and the output shapes. \n",
    "2. The functions `affine_forward`, `affine_backward` in **./utils/layer_funcs.py**. \n",
    "\n",
    "Replace the $\\color{cyan}{[eq.]}$ inside the equations with your own answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: If you're not familiar with LaTeX-style eqautions, see https://latex-tutorial.com/tutorials/amsmath/ for tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>Hint</strong></font>: Arrange the terms so that the shape of the gradient matches the shape of the differentiation variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the upstream gradient from the final output $Z$ w.r.t the layer output $Y$ is $\\nabla_Y Z = g \\in R^{N \\times M}$, show the backward pass[1]: \n",
    "\n",
    "$$\n",
    "\\nabla_X Z = {\\color{cyan}{[eq.]}} \\in R^{\\color{cyan}{[eq.]}}, \\quad\n",
    "\\nabla_W Z = {\\color{cyan}{[eq.]}} \\in R^{\\color{cyan}{[eq.]}}, \\quad\n",
    "\\nabla_b Z = {\\color{cyan}{[eq.]}} \\in R^{\\color{cyan}{[eq.]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://cs231n.stanford.edu/handouts/linear-backprop.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "2AfAMrOZtrpb",
    "outputId": "af228bb7-f1eb-41bf-e9f5-c3a6de6cdbd5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_funcs import affine_forward\n",
    "from utils.layer_funcs import affine_backward\n",
    "\n",
    "# generate data for checking\n",
    "x = X_dev\n",
    "w = np.random.rand(x.shape[1],100)\n",
    "b = np.random.rand(100)\n",
    "dout = np.ones((x.shape[0],100))\n",
    "\n",
    "## Affine function: H = W*X + b\n",
    "out = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, x, w, b)\n",
    "\n",
    "## check your implementation using the tf.gradients_function()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def affine_layer(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = affine_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check, dw_check, db_check = dx_tf.numpy(), dw_tf.numpy(), db_tf.numpy()\n",
    "\n",
    "## Print validation results\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $ReLU(\\cdot)$ is an element-wise mapping, consider the function admits a scaler input $x$, the forward pass: \n",
    "\n",
    "$$y = \\max (x, 0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LL0DqZ_ftrpd"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete \n",
    "\n",
    "1. The following equation. \n",
    "2. The functions `relu_forward`, `relu_backward` in **./utils/layer_funcs.py**.\n",
    "\n",
    "Replace the $\\color{cyan}{[eq.]}$ inside the equations with your own answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the upstream gradient from the final output $Z$ w.r.t the layer output $y$ is $\\nabla_y Z = g$, show the backward pass: \n",
    "\n",
    "$$\n",
    "\\nabla_x Z = \\begin{cases}\n",
    "{\\color{cyan}{[eq.]}} & \\text{if } x > 0 \\\\\n",
    "{\\color{cyan}{[eq.]}} & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nYDT8Idatrpe",
    "outputId": "a1db7202-1da9-45f5-bde1-ee42b623029f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_funcs import relu_forward\n",
    "from utils.layer_funcs import relu_backward\n",
    "\n",
    "## Activation layers -- Here we introduce ReLU activation function\n",
    "## since it is the most commonly used in computer vision problems.\n",
    "## However, you can also try to implement \n",
    "## other activation functions like sigmoid, tanh etc.\n",
    "x = X_dev\n",
    "dout = np.ones(x.shape)\n",
    "\n",
    "## ReLU\n",
    "out = relu_forward(x)\n",
    "dx = relu_backward(dout, x)\n",
    "\n",
    "## check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    out_tf = tf.nn.relu(x_tf)\n",
    "    grad_gt = tape.gradient(out_tf, x_tf)\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check = grad_gt.numpy()\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax cross-entropy layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the softmax function $\\sigma (\\cdot): R^K \\to \\Delta_K$ and the cross entropy $H (\\cdot): \\Delta_K \\times \\Delta_K \\to R_+$: \n",
    "\n",
    "$$\n",
    "\\sigma (s) = \\frac{e^s}{\\sum_k e^{s_i}}, \\quad\n",
    "H(p, q) = -p^T \\log q\n",
    "$$\n",
    "\n",
    "Assume the predictions $X \\in R^{N \\times K}$ ($x_i \\in X$ is the logit over all classes in $R^K$ for the $i$-th data sample, $\\sigma_i = \\sigma (x_i) \\in \\Delta_K$) and the ground truth $P \\in R^{N \\times K}$ ($p_i \\in P$ is the one-hot encoding of the ground truth class in $R^K$), the softmax cross entropy loss: \n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_i L_i = \\frac{1}{N} \\sum_i H(p_i, \\sigma_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrSXJOdktrph"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete \n",
    "\n",
    "1. The following equations and the output shapes. \n",
    "2. The functions `softmax_loss` in **./utils/layer_funcs.py**. \n",
    "\n",
    "Replace the $\\color{cyan}{[eq.]}$ inside the equations with your own answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>Hint</strong></font>: \n",
    "Refer to the notations and derivation in [***Task 1 Part 2***](./task1-basic_classifiers.ipynb#Part-2:-Softmax-Classifier) where we worked on $\\nabla_W L$ (please pay attention to the changes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the backward pass. You can do this in two steps: \n",
    "\n",
    "- Using $p_i \\in R^K$ and $\\sigma_i \\in R^K$, show the gradient of $L_i$ w.r.t $x_i$: \n",
    "\n",
    "$$\\nabla_{x_i} L_i = {\\color{cyan}{[eq.]}} \\in R^{\\color{cyan}{[eq.]}}$$\n",
    "\n",
    "- Using $P = (p_1, \\dots, p_N) \\in R^{N \\times K}$ and $\\sigma = (\\sigma_1, \\dots, \\sigma_N) \\in R^{N \\times K}$, show the gradient of $L$ w.r.t $X$: \n",
    "\n",
    "$$\\nabla_X L = {\\color{cyan}{[eq.]}} \\in R^{\\color{cyan}{[eq.]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "rG2_zSsjtrpi",
    "outputId": "5b8fd3ce-0175-4aa2-c29d-b0925dcf2f42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_funcs import softmax_loss\n",
    "\n",
    "## generate some random data for testing\n",
    "x = np.random.rand(100, 10)\n",
    "y = np.argmax(x, axis=1)\n",
    "\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "## check by tf.GradientTape.gradients()\n",
    "\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "y_tf = tf.Variable(y, name='y')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    loss_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x_tf, labels=tf.one_hot(y_tf, 10)))\n",
    "    dx_tf = tape.gradient(loss_tf, x_tf)\n",
    "\n",
    "loss_check = loss_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "## Print validation result\n",
    "print(\"Is loss correct? {}\".format(np.allclose(loss, loss_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement an affine layer, and then a dense layer on top of that. \n",
    "\n",
    "* input >> AffineLayer >> output\n",
    "\n",
    "```\n",
    "    class AffineLayer:\n",
    "        __init__:\n",
    "            params - weights and bias\n",
    "            cache - intermeidate results for back propagation\n",
    "            gradients - gradients of the parameters for optimization\n",
    "        feedforward: forward pass\n",
    "        backward: backward pass\n",
    "        update_layer: update layer parameters\n",
    "```\n",
    "\n",
    "* input >> AffineLayer >> ReLU >> output\n",
    "\n",
    "```\n",
    "    class DenseLayer:\n",
    "        __init__:\n",
    "            affine - an affine layer\n",
    "            activation: activation function\n",
    "        feedforward: forward pass\n",
    "        backward: backward pass\n",
    "        update_layer: update layer parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhIgKu9ptrpl"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete function `AffineLayer` in **./utils/layer_utils.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "Se8aWo7Ktrpm",
    "outputId": "803e04f9-7654-4ad4-8050-90e0f0e6a2c5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_utils import AffineLayer\n",
    "\n",
    "## Affine\n",
    "test_affine = AffineLayer(input_dim=X_train.shape[1], output_dim=100)\n",
    "w, b = test_affine.params['W'], test_affine.params['b']\n",
    "\n",
    "## Data for correctness check\n",
    "x = X_dev\n",
    "dout = np.ones((x.shape[0], 100))\n",
    "\n",
    "out = test_affine.feedforward(x)\n",
    "dx = test_affine.backward(dout)\n",
    "dw, db = test_affine.gradients['W'], test_affine.gradients['b']\n",
    "\n",
    "## check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def affine_layer(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = affine_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "    \n",
    "out_check = out_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "dw_check = dw_tf.numpy()\n",
    "db_check = db_tf.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2MhDHIJtrpo"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete function `DenseLayer` in **./utils/layer_utils.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "G8MgO2Gztrpq",
    "outputId": "5047dedc-a265-437f-882e-24adfd691074",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "## First, let's make a dense layer\n",
    "from utils.layer_utils import DenseLayer\n",
    "\n",
    "## Affine + ReLU\n",
    "test_dense = DenseLayer(input_dim=X_train.shape[1], output_dim=100)\n",
    "w, b = test_dense.params['W'], test_dense.params['b']\n",
    "\n",
    "## Data for correctness check\n",
    "x = X_dev\n",
    "dout = np.ones((x.shape[0], 100))\n",
    "\n",
    "out = test_dense.feedforward(x)\n",
    "dx = test_dense.backward(dout)\n",
    "dw, db = test_dense.gradients['W'], test_dense.gradients['b']\n",
    "\n",
    "## check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def dense_layer(x, w, b):\n",
    "    return tf.nn.relu(tf.matmul(x, w) + b)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = dense_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "dw_check = dw_tf.numpy()\n",
    "db_check = db_tf.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Two Layer Network (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9cJLCPstrpt"
   },
   "source": [
    "Complete the class **TwoLayerNet** in **./utils/classifiers/twolayernet.py**. Through this experiment, you will create a two-layer neural network and learn about the backpropagation mechanism. The network structure is like \n",
    "\n",
    "* input >> DenseLayer >> AffineLayer >> softmax loss >> output\n",
    "\n",
    "```\n",
    "    class TwoLayerNet:\n",
    "        __init__: \n",
    "            - layers: a dense layer and an affine layer\n",
    "        forward: forward pass\n",
    "        loss: cross entropy loss and gradients\n",
    "        step: a single step update of all weights and bias by SGD.\n",
    "        predict: output result (classification accuracy) based on input data\n",
    "        save_model: return the parameters of the network\n",
    "        update_model: update model weights (by calling layer.update_layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison between SGD and SGD with momentum.\n",
    "\n",
    "* Stochastic gradient descent - SGD\n",
    "\n",
    "$$w \\gets w - \\alpha \\nabla_w L$$\n",
    "\n",
    "* SGD with momentum\n",
    "\n",
    "$$v \\gets \\beta v + \\alpha \\nabla_w L,\\quad w \\gets w - v$$\n",
    "\n",
    "where $\\alpha$ is the learning rate (step size) and $\\beta$ is the momentum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBFtQx5Utrpu"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete class `TwoLayerNet` in **./utils/classifiers/twolayernet.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1QlpmRrEtrpv",
    "outputId": "2dfb94b6-1d2d-4bf6-efce-0d54c7725cc8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## Define a model\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=100, num_classes=20, reg=1e-4)\n",
    "W1, b1 = model.layer1.params['W'], model.layer1.params['b']\n",
    "W2, b2 = model.layer2.params['W'], model.layer2.params['b']\n",
    "## Feedforward\n",
    "y_score = model.forward(X_dev)\n",
    "## Backprogation -- Finish loss function and gradients calculation in TwoLayerNet\n",
    "loss = model.loss(y_score, y_dev)\n",
    "\n",
    "## Check loss by tensorflow\n",
    "x_tf = tf.Variable(X_dev, dtype=tf.float32)\n",
    "y_tf = tf.Variable(y_dev, dtype=tf.uint8)\n",
    "\n",
    "W1_tf = tf.Variable(W1.astype('float32'))\n",
    "b1_tf = tf.Variable(b1.astype('float32'))\n",
    "W2_tf = tf.Variable(W2.astype('float32'))\n",
    "b2_tf = tf.Variable(b2.astype('float32'))\n",
    "h1_tf = tf.nn.relu(tf.matmul(x_tf, W1_tf))\n",
    "h2_tf = tf.matmul(h1_tf, W2_tf) + b2_tf\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=h2_tf, labels=tf.one_hot(y_tf, 20))\n",
    "L2_loss = tf.nn.l2_loss(W1_tf) + tf.nn.l2_loss(W2_tf)\n",
    "loss_tf = tf.reduce_mean(cross_entropy) + 1e-4 * L2_loss \n",
    "\n",
    "loss_check=loss_tf.numpy()\n",
    "    \n",
    "## Print validation result\n",
    "print(\"Is loss correct? {}\".format(np.allclose(loss, loss_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WjSapZXtrpy"
   },
   "source": [
    "### Train a two-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnEd1Z9Wtrpz"
   },
   "source": [
    "#### Import functions for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "et5ZTXBktrpz"
   },
   "outputs": [],
   "source": [
    "from utils.train_funcs import train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2Nbd9iotrp2"
   },
   "source": [
    "#### Start training\n",
    "We have provide you the `train` function in **./utils/train_func.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eOIqjSUCtrp2",
    "outputId": "8cc8db83-6caa-4357-e086-87801f6fabf1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## TODO: Use previous layers to create a two layer neural network\n",
    "## input->(affine->activation)->(affine->softmax)->output\n",
    "## The recommended activation function is ReLU. And you can \n",
    "## also make a comparison with other activation function to see\n",
    "## any difference.\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=400, num_classes=20, reg=1e-4, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 500\n",
    "lr = 5e-4\n",
    "verbose = True\n",
    "train_acc_hist, val_acc_hist = train(\n",
    "    model, X_train, y_train, X_val, y_val, num_epoch=num_epoch, batch_size=batch_size, \n",
    "    learning_rate=lr, verbose=verbose\n",
    ")\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID-4_nLqtrp5"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Plot training and validation accuracy history of each epoch. Remember to add a legend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: plot the accuracy history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42xHlu5utrp9"
   },
   "source": [
    "#### Visulize the weight variable in the first layer.\n",
    "\n",
    "Visualization of the intermediate weights can help you get an intuitive understanding of how the network works, especially in  Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFr5Kvrmtrp9"
   },
   "outputs": [],
   "source": [
    "from utils.display_funcs import visualize_pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "OrusGTkltrqA",
    "outputId": "6a87d009-7825-4795-b23d-95ba44a5cf8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = model.layer1.params['W']\n",
    "pics = weights.reshape(1, X_train_raw.shape[1], X_train_raw.shape[2], -1).transpose(3, 1, 2, 0)\n",
    "## visualization\n",
    "visualize_pics(pics, cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2s4-QzTHtrqD"
   },
   "source": [
    "### Get test accuracy greater than 80%\n",
    "\n",
    "For this part, you need to train a better two-layer net. The requirement is to get test accuracy better than 80%. If your accuracy is lower, for each 1% lower than 80%, you will lose 1 point (There are totally 10 points for this part).\n",
    "\n",
    "Here are some recommended methods for improving the performance. Feel free to try any other method as you see fit.\n",
    "\n",
    "1. Hyperparameter tuning: reg, hidden_dim, lr, learning_decay, num_epoch, batch_size, weight_scale.\n",
    "2. Adjust training strategy: Randomly select a batch of samples rather than selecting them orderly. \n",
    "3. Try new optimization methods: Now we are using SGD, you can try SGD with momentum, adam, etc.\n",
    "4. Early-stopping.\n",
    "5. Good (better) initial values for weights in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK0pOcTLtrqE"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfTHBopktrqE"
   },
   "outputs": [],
   "source": [
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "# TODO: Use previous layers to create a two layer neural network.\n",
    "# Try several solutions and report the best performing one.\n",
    "# input->(affine->activation)->(affine->softmax)->output\n",
    "# The recommended activation function is ReLU. You can \n",
    "# make a comparison with other activation functions to see\n",
    "# the differences.\n",
    "#\n",
    "# You will need to execute code similar to the code below, using your parameter specs:\n",
    "#    model = TwoLayerNet(input_dim=TBD, hidden_dim=TBD, num_classes=TBD, reg=TBD, weight_scale=TBD)\n",
    "#    num_epoch = TBD\n",
    "#    batch_size = TBD\n",
    "#    lr = TBD\n",
    "#    verbose = TBD\n",
    "#    train_acc_hist, val_acc_hist = train(TBD)\n",
    "#    test(TBD, TBD, TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qI9Zc9cItrqJ"
   },
   "source": [
    "#### <font color=\"red\"><strong>TODO</strong></font>: Show your best result, including accuracy and weights of the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: plot training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Visualize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7SAy1J1trqP"
   },
   "source": [
    "### Save your best model in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9iMeOm1trqQ"
   },
   "outputs": [],
   "source": [
    "## Create \"save_model\" folder if it does not exist\n",
    "save_dir = \"./save_models/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "## Save your model\n",
    "save_params = model.save_model()\n",
    "with open(\"./save_models/best_model.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(save_params, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XzKlA_1trqT"
   },
   "outputs": [],
   "source": [
    "## Load your model\n",
    "with open(\"./save_models/best_model.pkl\", \"rb\") as input_file:\n",
    "   loaded_model = pickle.load(input_file)\n",
    "\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=300, num_classes=20)\n",
    "model.update_model(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6DUwt8OtrqV"
   },
   "source": [
    "## Part 3: Multilayer Network (10%)\n",
    "\n",
    "Complete the class **MLP** in **./utils/classifiers/mlp.py**. It should allow arbitrary settings for the number of hidden layers as well as the number of hidden neurons in each layer. **MLP** has a similar structure as a **TwoLayerNet** network.\n",
    "\n",
    "```\n",
    "class MLP:\n",
    "    functions: __init__, loss, step, predict, check_accuracy\n",
    "    variables: layers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the class `MLP` in **./utils/classifiers/mlp.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "1C7yH-HBtrqW",
    "outputId": "718a8627-54d1-456f-afb4-3239c3564191",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.mlp import MLP\n",
    "\n",
    "## Use a sequence of layers to create a multiple layer neural network\n",
    "## input->(affine->activation)-> ... ->(affine->activation)->(affine->softmax)->output\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200, 100], num_classes=20, reg=0.5, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 128\n",
    "lr = 5e-3\n",
    "verbose = False\n",
    "train_acc_hist, val_acc_hist = train(\n",
    "    model, X_train, y_train, X_val, y_val, \n",
    "    num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, \n",
    "    optim='SGD', momentum=0.9, verbose=verbose\n",
    ")\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Plot training and validation accuracy history of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2-mlp_eager.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
